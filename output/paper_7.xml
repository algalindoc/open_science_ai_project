<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMAGECHAIN: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Danae</forename><surname>Sánchez Villegas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ingo</forename><surname>Ziegler</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IMAGECHAIN: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CA8839437088203B65AEDB4EC62D5FD4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-03-04T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating images independently. This work introduces IMAGECHAIN, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visual sequences as a multi-turn conversation. In IMAGECHAIN, images are interleaved with corresponding textual descriptions to form a controlled dialogue that explicitly captures temporal dependencies and narrative progression. Our method optimizes for the task of next-scene description, where the model generates a context-aware description of an upcoming scene based on preceding visual and textual cues. We demonstrate that our approach improves performance on the nextscene description task -achieving an average improvement from 3.7% to 19% in SimRate, a metric that quantifies semantic similarity to human-annotated ground truths. Moreover, IM-AGECHAIN achieves robust zero-shot out-ofdomain performance in applications ranging from comics to robotics. Extensive experiments validate that instruction-tuning in a multimodal, multi-turn conversation design is key to bridging the gap between static image understanding and temporally-aware reasoning. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal large language models (MLLMs) such as GPT-4V <ref type="bibr">(Achiam et al., 2023)</ref>, MM1 <ref type="bibr" target="#b31">(McKinzie et al., 2025)</ref>, and LLaVA-NeXT <ref type="bibr" target="#b29">(Liu et al., 2024)</ref> have demonstrated impressive reasoning capabilities by integrating text and image inputs, advancing the state of visual-language understanding <ref type="bibr" target="#b49">(Zhang et al., 2024a)</ref>. Standard tasks like image Figure <ref type="figure">1</ref>: Illustration of the proposed next-scene description task. Models and techniques that process images independently, such as LLaVA-NeXT and standard finetuning, fail to capture the correct progression, leading to errors. In contrast, IMAGECHAIN explicitly models sequences as multi-turn conversations, enabling it to generate a more accurate description, closely aligning with the human-annotated ground truth.</p><p>captioning and visual question answering (VQA) have driven significant progress in recognizing objects, attributes, and their relationships within individual images <ref type="bibr" target="#b37">(Stefanini et al., 2022;</ref><ref type="bibr" target="#b36">Srivastava et al., 2021;</ref><ref type="bibr" target="#b32">Narins et al., 2024;</ref><ref type="bibr" target="#b45">Wang, 2022)</ref>. However, many real-world applications such as storytelling <ref type="bibr" target="#b16">(Huang et al., 2016;</ref><ref type="bibr" target="#b42">Wang et al., 2020;</ref><ref type="bibr" target="#b28">Liu et al., 2023)</ref>, event comprehension <ref type="bibr" target="#b23">(Lei et al., 2020;</ref><ref type="bibr" target="#b4">Cheng et al., 2024)</ref>, and robotics (O <ref type="bibr">'Neill et al., 2024)</ref>, demand a deeper understanding of temporal and narrative progression across sequences of images.</p><p>Recent methods have extended MLLMs such as LLaVA-NeXT, Mantis <ref type="bibr" target="#b19">(Jiang et al., 2024a)</ref>, and Qwen2-VL <ref type="bibr" target="#b41">(Wang et al., 2024b)</ref> to handle multi-arXiv: <ref type="bibr">2502.19409v1 [cs.CV]</ref> 26 Feb 2025 image inputs. However, they typically process images independently or summarize entire scenes, rather than explicitly modeling the evolution of events over time. In contrast, sequential image reasoning requires a model to capture dependencies across frames to predict future actions. Figure <ref type="figure">1</ref> illustrates our proposed task, next-scene description, which consists of generating a text description of a visual scene based on a sequence of preceding frames and their corresponding descriptionsa challenge that existing models, such as LLaVA-NeXT, have yet to overcome.</p><p>To address this gap, we introduce IMAGECHAIN, an efficient framework that provides MLLMs with explicit sequential reasoning capabilities. By reformulating a visual sequence as a multi-turn conversation, IMAGECHAIN interleaves images with their corresponding text descriptions to build a sequential context for generating the next-scene description. Our approach achieves substantial improvements on similarity rate (SimRate), a metric that quantifies semantic similarity to human-annotated ground truths, across both in-domain and out-ofdomain tasks using only approximately 4,000 training samples. To support our method, we repurpose StoryBench <ref type="bibr" target="#b2">(Bugliarello et al., 2023)</ref>, a video dataset with human-annotated descriptions, to create and introduce StoryFrames -a high-quality, temporally coherent corpus tailored towards sequential image-text reasoning across different context lengths. StoryFrames provides annotated samples that enable IMAGECHAIN to efficiently adapt and learn robust temporal dependencies with minimal data. Our contributions are as follows:</p><p>• Framework: We introduce IMAGECHAIN, an image-to-text reasoning adaptation framework that models image sequences as multiturn conversations for generating next-scene descriptions, achieving an overall SimRate of 19% versus 3.7% for standard MLLMs.</p><p>• Robust Out-of-Domain Performance: In robotics, IMAGECHAIN achieves an F1 score of 27.1-almost double the 14.4 of standard fine-tuning -along with gains in structured settings such as comics. 2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evolution of Sequential Visual Reasoning</head><p>Visual storytelling emerged as an early effort to generate coherent narratives from image sequences <ref type="bibr" target="#b16">(Huang et al., 2016)</ref>. Initial approaches relied on convolutional neural networks <ref type="bibr" target="#b22">(LeCun et al., 1995)</ref> for visual feature extraction paired with recurrent neural networks <ref type="bibr" target="#b13">(Hochreiter, 1997)</ref> for narrative generation <ref type="bibr" target="#b12">(Gonzalez-Rico and Fuentes-Pineda, 2018;</ref><ref type="bibr" target="#b21">Kim et al., 2018)</ref>. Although these methods demonstrated that visual and textual information could be integrated to produce compelling stories, they relied on overly broad scene summarization techniques <ref type="bibr" target="#b14">(Hong et al., 2020;</ref><ref type="bibr" target="#b42">Wang et al., 2020)</ref>, rather than explicitly modeling the temporal dependencies required to predict future events. With the introduction of MLLMs such as GPT-4V <ref type="bibr">(Achiam et al., 2023</ref><ref type="bibr">), MM1 (McKinzie et al., 2025)</ref>, and LLaVA-NeXT <ref type="bibr" target="#b29">(Liu et al., 2024)</ref>, significant progress has been made in static image understanding for tasks like image captioning and visual question answering <ref type="bibr" target="#b28">(Liu et al., 2023;</ref><ref type="bibr" target="#b27">Lin and Chen, 2024;</ref><ref type="bibr" target="#b50">Zhang et al., 2025b)</ref>. Despite these successes, many MLLMs remain optimized for static or non-sequential multi-image inputs, limiting their ability to capture temporal dynamics. This limitation motivates our work to develop a method that explicitly models temporal dependencies across image sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenges in Context Length and</head><p>Generalization A critical challenge in sequential visual reasoning lies in effectively handling variable-length temporal contexts <ref type="bibr" target="#b53">(Zhou et al., 2024b)</ref>. Many existing models are trained on short or fixed-length sequences and thus struggle when presented with complex temporal spans or variable length contexts <ref type="bibr" target="#b39">(Thawakar et al., 2025)</ref>. Furthermore, models show particular difficulties handling subtle temporal dependencies and structured event progressions where actions follow constrained logical sequences, such as in comics or robotics <ref type="bibr" target="#b44">(Wang et al., 2024c)</ref>. A recent analysis suggests that MLLMs rely on surface-level cues <ref type="bibr" target="#b53">(Zhou et al., 2024b)</ref>, leading to performance degradation when processing extended contexts or adapting to novel scenarios <ref type="bibr" target="#b17">(Imam et al., 2025)</ref>. These challenges underscore the need for approaches that capture variablerange temporal dependencies and generalize more robustly across diverse domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Instruction Tuning Over Multi-Turn Conversations</head><p>Instruction tuning <ref type="bibr" target="#b35">(Sanh et al., 2022;</ref><ref type="bibr" target="#b46">Wei et al., 2022)</ref> enhances LLMs by improving their ability to follow general task-agnostic directives while requiring limited training data <ref type="bibr" target="#b52">(Zhou et al., 2024a)</ref>. This methodology diverges from conventional finetuning approaches by exposing the model to varied instructional formulations, which ensures that the emphasis lies on adhering to directives rather than on task-specific details. <ref type="bibr" target="#b51">(Zhang et al., 2024b)</ref>.</p><p>Empirical evaluations demonstrate that instructiontuned variants consistently outperform competing baselines in output quality across open-ended <ref type="bibr" target="#b18">(Jha et al., 2023)</ref>, knowledge <ref type="bibr" target="#b20">(Jiang et al., 2024b)</ref>, and reasoning <ref type="bibr" target="#b38">(Tang et al., 2024)</ref> tasks. Instruction tuning over multi-turn conversations extend this approach by introducing sequential dependencies between interactions, where each turn builds on prior user-model exchanges <ref type="bibr" target="#b48">(Zhang et al., 2025a)</ref>. Despite these advances in LLMs, such multi-turn conversational techniques have not yet been applied to multimodal settings.</p><p>IMAGECHAIN extends the multi-turn conversational paradigm to MLLMs for improved sequential visual reasoning by leveraging the strengths of instruction tuning and multi-turn interactions for integrating sequential visual data. Rather than relying on emerging user-model interactions, we explicitly structure the conversation as a fixed sequence <ref type="bibr" target="#b40">(Wang et al., 2024a)</ref>. We interleave visual embedding tokens with scene descriptions to build a controlled context that emphasizes temporal dependencies. Each turn poses a targeted question about an upcoming scene, with the expected response being a text description. We call this task next-scene description (Figure <ref type="figure" target="#fig_0">2</ref>), where the goal is to generate an accurate description of a visual scene based on preceding frames and annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IMAGECHAIN: Optimizing MLLMs for Sequential Image-to-Text Reasoning</head><p>IMAGECHAIN enhances an MLLM's ability to reason over sequential image data by optimizing the task of next-scene description. We assume a multimodal language model that can process both text and visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setup</head><p>Let S be a story represented as a sequence of scenes S = ⟨s t ⟩ T t=1 . Each scene s consists of a sequence of frames V = ⟨v k ⟩ K k=1 and a textual description D that corresponds to the entire scene, expressed &lt;s&gt; USER: What is happening in this image? &lt;Image&gt;&lt;image&gt;&lt;/Image&gt; ASSISTANT: A rider wearing a black t-shirt is riding a bicycle on a brown surface while two people are sitting on their bicycle behind the rider. &lt;/s&gt; &lt;s&gt; USER: What is happening in the next image? &lt;Image&gt;&lt;image&gt;&lt;/Image&gt; ASSISTANT: The rider wearing a black t-shirt jumps on the trampoline. &lt;/s&gt; &lt;s&gt; USER: What is happening in the next image? &lt;Image&gt;&lt;image&gt;&lt;/Image&gt; ASSISTANT: The rider wearing a black t-shirt gets disbalanced and falls. &lt;/s&gt; &lt;s&gt; USER: What is happening in the next image? &lt;Image&gt;&lt;image&gt;&lt;/Image&gt; ASSISTANT:</p><p>Figure <ref type="figure">3</ref>: Multi-turn conversation design for a story with four scenes, where each turn corresponds to a scene. A turn begins with a user question and ends with the assistant's response. The context includes three completed turns (i.e., three scenes), along with the next user question and the corresponding visual cue, which are used to generate the next-scene description.</p><p>as s = (V, D). The number of scenes in a story and the number of frames per scene can vary across different stories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Obtaining Visual Scene Representations</head><p>Each sequence of frames V is transformed into a fixed-size representation to be paired with the sole description D. Given a visual encoder f θ (•), each frame v k is mapped to a feature vector as</p><formula xml:id="formula_0">z k = f θ (v k ), where z k ∈ R d .</formula><p>We then average the feature vectors z k over all frames to compute the scene-level visual representation as follows: V = 1 K K k=1 z k . The averaged feature vector V serves as a fixed-size summary of the visual content in V , which is then paired with the corresponding textual description D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Turn Conversation Construction</head><p>To capture the sequential nature of each story, we frame every scene as part of a multi-turn conversation between the user and the model. For a given story S, let τ ∈ {1, 2, . . . , T } denote the turns, where each turn τ is a triple Q τ , Vτ , D τ . Q τ is a predefined question string that asks from a user perspective "What is happening in this image?" if τ = 1 to start the conversation, or "What is happening in the next image?" for all τ &gt; 1 to proceed to the next turn in the conversation. Therefore, each story S can be represented by a conversation context C of T turns:</p><formula xml:id="formula_1">C = (Q 1 , V1 , D 1 ), . . . , (Q T , VT , D T ) . (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Instruction Fine-Tuning Objective</head><p>We fine-tune the model using standard supervised next-token prediction over the multi-turn conversation context C. Let W C = ⟨w 1 , w 2 , . . . , w N ⟩ denote the concatenated sequence of all text tokens in the context C. Our training goal for an individual story is to minimize the cross-entropy loss over all text tokens w i conditioned on all preceding text tokens and the visual context:</p><formula xml:id="formula_2">L IC = − N i=1 log p w i w 1:i−1 , Vτ T τ =1 . (2)</formula><p>The visual embeddings V are provided only for conditioning and do not contribute to the loss. By enforcing a multi-turn structure that explicitly exposes the model to sequential context at each turn, minimizing L IC improves the model's temporal reasoning over sequential image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Next-Scene Description Generation</head><p>To generate the next-scene description Dτ at turn τ , we condition the instruction-tuned model on the completed turns {1, 2, . . . , τ − 1}, plus the current question Q τ and the embedding Vτ . Note that the next-scene description D τ is withheld:</p><formula xml:id="formula_3">C τ = (Q 1 , V1 , D 1 ), . . . , (Q τ −1 , Vτ−1 , D τ −1 ), (Q τ , Vτ , □) . (3)</formula><p>At inference time, the model auto-regressively generates the next-scene description Dτ based on the provided multi-turn context C τ . An example context with τ = 4 turns is illustrated in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The StoryFrames Dataset</head><p>We adapt the StoryBench dataset <ref type="bibr" target="#b2">(Bugliarello et al., 2023)</ref> to create StoryFrames -a dataset specifically designed for the next-scene description task on sequential visual reasoning. StoryFrames repurposes human-annotated temporal segments from three distinct sources: Oops <ref type="bibr" target="#b10">(Epstein et al., 2020)</ref>, which captures unexpected actions; DiDeMo (Anne <ref type="bibr" target="#b1">Hendricks et al., 2017)</ref>, which provides event grounding through natural language; and UVO <ref type="bibr" target="#b43">(Wang et al., 2021)</ref>, which focuses on object-centric reasoning in video sequences. In this dataset, each story represents a single "sample" consisting of a sequence of scenes. Every scene is defined by human-annotated start and end points, accompanied by a textual description covering its duration. Each scene is further divided into multiple extracted frames, forming a hierarchical structure that supports detailed sequential reasoning.</p><p>Frame Extraction. To extract frames, we implement an adaptive frame sampling strategy that adjusts the number of frames based on the total duration of each story. For stories up to 5 seconds, we extract 8 frames; for 5 to 10 seconds, 12 frames; for 10 to 15 seconds, 15 frames; for 15 to 30 seconds, 20 frames; and for stories exceeding 30 seconds, 25 frames. This approach ensures broad temporal coverage while maintaining manageable computational requirements. Moreover, to preserve narrative continuity and prevent overlap between adjacent scenes, a 0.2-second temporal offset is introduced between consecutive action segments.</p><p>Frame Allocation Across Scenes. Within each story, frame allocation to individual scenes is performed proportionally to the duration of each scene, while ensuring that every scene receives at least two frames. Formally, for a scene s with duration m s in a story S of total duration M and given a total frame budget F , the number of frames allocated is computed as follows: K s = max 2, ms M • F . StoryFrames Organization. StoryFrames comprises 8,881 sequences and is organized into distinct subsets based on the number of scenes per story (Figure <ref type="figure" target="#fig_1">4</ref>) to enable evaluation of models across varying context lengths and complexity.</p><p>Dataset Splits. For sequences containing 2 to 7 scenes, we maintain an 80-20 train-validation split, while single-scene sequences and those with 8 or more scenes are reserved for further validation to assess generalization to extreme cases. Figure <ref type="figure" target="#fig_1">4</ref> visualizes the dataset distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Models</head><p>We evaluate a series of model configurations ranging from unmodified baselines to fully fine-tuned variants. This systematic evaluation allows us to isolate the contributions of fine-tuning, visual context, and structured dialogue, thereby assessing the necessity and effectiveness of IMAGECHAIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLLM:</head><p>The baseline MLLM, designed to assess task performance without any task-specific modifications.</p><p>MLLM-ICL: An in-context learning baseline that receives three demonstrations of the task, designed to evaluate whether sequential image reasoning can be learned solely through seeing examples.</p><p>MLLM-FT: MLLM with standard fine-tuning on the StoryFrames dataset, allowing a direct comparison with our proposed method.</p><p>VisualContext: A variant fine-tuned using only the visual context from preceding actions, i.e., all textual descriptions are omitted, enabling us to assess the contribution of text in the prediction task.</p><p>FinalScene: A variant that represents image captioning by fine-tuning the model to predict the next description based solely on the final visual action, designed to analyze whether incorporating context improves performance.</p><p>IMAGECHAIN-NoFT: Our approach that employs a multi-turn conversation structure but without any fine-tuning, isolating the effect of prompting with the structured dialogue format.</p><p>IMAGECHAIN: The complete method that integrates multi-turn prompting with fine-tuning on StoryFrames, explicitly modeling sequential dependencies to enhance next-scene descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset and Context Length Splits</head><p>To evaluate the impact of temporal context on generating next-scene descriptions, we use stories from the StoryFrames dataset with 2 to 7 scenes. In our setup, the model is given a varying number of preceding scenes to predict the textual description for the current one. This analysis reflects real-world applications that require reasoning over differing lengths of context. We define 3 evaluation settings:</p><p>C2: The model uses one preceding scene (a total of 2 scenes, including the current one).</p><p>C3: Here, the model uses two preceding scenes (totaling 3 scenes).</p><p>C4-7: In this setting, the model receives a longer sequence, with three to six preceding scenes (corresponding to sequences of 4 to 7 scenes in total).</p><p>C2-7: The model is exposed to the full range of context lengths available (i.e., from 2 to 7 scenes). This range captures both short-term dependencies and more extended temporal structures and is designed to improve the model's ability to generalize across varying context lengths.</p><p>By comparing model performance across the fixed settings (C2, C3, C4-7, C2-7), we are able to isolate the contribution of context length to nextscene description and provide insights on optimizing for specific context lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training and Implementation Details</head><p>All of our model configurations share a common experimental backbone based on LLaVA-v1.6-Vicuna-7B <ref type="bibr" target="#b29">(Liu et al., 2024)</ref>, which aligns Vicuna <ref type="bibr">(Chiang et al., 2023)</ref> and CLIP <ref type="bibr" target="#b34">(Radford et al., 2021)</ref> into an MLLM. We fine-tune the models using low-rank adaptation <ref type="bibr">(Hu et al., 2021, LoRA)</ref> in combination with DeepSpeed ZeRO-3, running approximately 1-2 hours per model on 8 NVIDIA A100 GPUs. The baseline training is conducted over 3 epochs on the StoryFrames dataset with a per-device batch size of 4 and gradient accumulation of 1. LoRA is configured with a rank r = 128 and α = 256. A learning rate of 2 × 10 −5 is applied, with a cosine learning rate schedule and a warmup ratio of 3%. For optimization, the adaptive momentum optimizer with decoupled weight decay <ref type="bibr" target="#b30">(Loshchilov and Hutter, 2019)</ref> is used. For IMAGECHAIN trained on C2 and C3 we adjust the number of training epochs to 5, and to 7 epochs when trained on C4-7 due to the smaller training sample sizes. All other training settings remain unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation</head><p>LLM-as-a-Judge. To quantitatively assess the quality of the generated descriptions, we use sim-ilarity rate (SimRate), an adapted version of win rate <ref type="bibr" target="#b6">(Chiang et al., 2024)</ref>, as a metric. Given the inherent variability in how visual events can be described literally, conventional metrics relying on n-gram overlap are inadequate for capturing semantic equivalence <ref type="bibr" target="#b7">(Culy and Riehemann, 2003;</ref><ref type="bibr" target="#b3">Bulian et al., 2022)</ref>. Instead, we follow recent literature <ref type="bibr" target="#b25">(Li et al., 2024</ref><ref type="bibr" target="#b24">(Li et al., , 2025) )</ref> and adopt LLMs as evaluators <ref type="bibr" target="#b9">(Eldan and Li, 2023)</ref>. For our generated next-scene description Dτ , Llama 3 70B <ref type="bibr" target="#b8">(Dubey et al., 2024)</ref> determines whether or not it conveys a similar meaning to the ground truth scene description D τ , effectively serving as a proxy for human evaluators <ref type="bibr" target="#b26">(Li et al., 2023;</ref><ref type="bibr" target="#b6">Chiang et al., 2024)</ref>. The prompt used for this framework is detailed in Appendix A. The overall SimRate follows as the fraction of comparisons where model-generated descriptions are judged semantically similar to the human-annotated ground truth descriptions.</p><p>Out-of-Domain Generalization. To further assess model generalization, we evaluate performance on several out-of-domain datasets that target diverse sequential reasoning challenges. Specifically, we test on three datasets derived from the Mementos benchmark <ref type="bibr" target="#b44">(Wang et al., 2024c)</ref>. Comics, which features wordless multi-panel comics; Daily-Life (DL) <ref type="bibr" target="#b47">(Xiao et al., 2021)</ref>, consisting of videos depicting everyday activities; and Robo (O <ref type="bibr">'Neill et al., 2024)</ref>, which contains robotics tasks. For evaluation, behavioral cues, (e.g., key verbs or verb phrases, are extracted from generated descriptions using GPT-4V <ref type="bibr">(Achiam et al., 2023)</ref> and compared against human annotations using F1 score.<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Explicit Sequence Modeling Improves Temporal Reasoning. Training on Longer Contexts Improves Sequential Reasoning. Figure <ref type="figure" target="#fig_2">5</ref> shows the models performance for fine-tuned models trained on different context lengths. IMAGECHAIN achieves the highest overall SimRate when evaluated on C2-6, particularly when trained on longer contexts, reaching 13.6% when trained on C4-7. This surpasses other models, such as MLLM-FT, which achieves 9.8% under the same training conditions. MLLM-FT excels in short contexts but struggles with longer dependencies, suggesting limitations in handling extended sequences without explicit sequence modeling. VisualContext, underperforms on longer sequences (7.7% trained on C4-7 and evaluated on C6), highlighting the benefit of including text descriptions for fine-tuning in long contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Out-of-domain Setup</head><p>Table <ref type="table" target="#tab_3">3</ref> compares IMAGECHAIN with various baselines and large-scale multimodal models (GPT-4V, Gemini <ref type="bibr" target="#b11">(Gemini Team et al., 2023)</ref>) across different domains. While GPT-4V and Gemini achieve the highest F1 scores, they operate at a much larger parameter and training data scale compared to our models, making direct comparisons challenging, but serve as a reference.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLLM-FT</head><p>In the image, a white bear is standing on the right side while holding a basketball in his hands and a deer is standing on the left side and look at the basketball. IMAGECHAIN In the image, there is a white bear on the right side wearing a red lower and holding a ball in its hands while a deer on the left side is moving its head. The white bear is standing and holding a ball in its hands and the deer is moving its head. The white bear is throwing the ball into the basketball hoop while a giraffe is standing and looking at the basketball hoop. The white bear is walking towards the basketball hoop and putting the ball into the hoop. suggesting that leveraging prior frames without explicit text descriptions can be effective in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, we introduced IMAGECHAIN, a framework designed to enhance MLLMs with explicit sequential reasoning capabilities, addressing a key limitation in existing MLLMs. Our results demonstrate substantial improvements over baseline models, with strong performance gains in both indomain and zero-shot out-of-domain settings. Our findings highlight the importance of instructiontuning within a multimodal, multi-turn conversation framework, suggesting promising directions for future work in refining temporal reasoning, scaling to more complex real-world scenarios and applications such as video understanding and robotics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>While IMAGECHAIN enhances sequential reasoning by structuring image sequences as multi-turn conversations, it has some limitations. We test our method on LLaVA-NeXT and average the feature vectors over all frames to compute the scenelevel visual representation, a simple yet effective approach. While future work can investigate performance in other models and can explore more advanced methods for combining feature vectors, this experimental setup is sufficient for evaluating our method. Scalability remains a challenge for longer sequences, as maintaining coherence across many turns may require more sophisticated techniques, which can be explored in future work. That said, our framework already models dependencies more effectively than standard MLLMs, making it a strong foundation for further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>This work uses publicly available licensed (CC BY 4.0) datasets consistent with their intended use (research) to ensure transparency and reproducibility.</p><p>While IMAGECHAIN enhances sequential reasoning, it may inherit biases from pre-trained models and datasets. Our framework is designed for research and development purposes, and we encourage responsible use, particularly in applications involving decision-making in sensitive domains such as healthcare and robotics. We acknowledge the use of Microsoft Copilot (https: //copilot.microsoft.com/) during the development of the coding experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Evaluation Prompt &lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;</head><p>You are a pattern-following assistant that can only answer with "Yes" or "No". Your goal is to determine whether a predicted caption conveys a similar enough meaning to the ground truth caption provided.&lt;|eot_id|&gt; &lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; ### Instruction: Determine if the predicted caption conveys a similar meaning to the ground truth caption.</p><p>### Ground truth caption: A man is riding a bicycle through a park.</p><p>### Predicted caption A person is cycling along a path in a park.</p><p>### Does the predicted caption convey a similar meaning to the ground truth caption (Yes or No)?&lt;|eot_id|&gt; &lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; Yes&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; Good job! Indeed, the predicted caption conveys a similar meaning to the ground truth. Both describe a person riding a bicycle in a park, even though different words are used. The core meaning is preserved.</p><p>### Instruction: Determine if the predicted caption conveys a similar meaning to the ground truth caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Ground truth caption:</head><p>A woman is sitting on a wooden bench in the park, reading a paperback novel under the shade of a tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Predicted caption:</head><p>A woman relaxes in a shaded area of the park, sitting on a bench while enjoying a book.</p><p>### Does the predicted caption convey a similar meaning to the ground truth caption (Yes or No)?&lt;|eot_id|&gt; &lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes&lt;|eot_id|&gt;&lt;|start_header_id|&gt; user&lt;|end_header_id|&gt;</head><p>Great! Although the wording differs, the predicted caption captures the essence of the ground truth. Both describe a woman sitting on a bench in a shaded park area, reading a book. While the predicted caption simplifies certain details, such as omitting the specific mention of the "paperback novel" and "under the shade of a tree," it still conveys the same overall scene and activity, making the meaning similar.</p><p>Let's do one more. Remember to answer with one word either "Yes" or "No".</p><p>### Instruction: Determine if the predicted caption conveys a similar meaning to the ground truth caption. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of multi-image sequential reasoning between a standard Multimodal Large Language Model (MLLM) and our proposed model IMAGECHAIN. The left side shows the output of MLLM, which fails to accurately describe the next event in the image sequence. The right side presents IMAGECHAIN, an image-to-text reasoning adaptation framework that models image sequences as multi-turn conversations, enabling a more accurate and temporally aware description of the next scene.</figDesc><graphic coords="3,70.87,70.86,453.55,204.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of scenes in our introduced Sto-ryFrames dataset. A story corresponds to one "sample", whereas multiple scenes make up one story.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Model performance (SimRate) for fine-tuned models trained on different context lengths (C2, C3, C4-7). IMAGECHAIN achieves the highest overall SimRate when trained on long contexts (C4-7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of next-scene descriptions. MLLM-FT (standard fine-tuning) describes static observations, while IMAGECHAIN captures sequential actions. Behavioral cues, i.e., key verbs, are highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>#Figure 7 :</head><label>7</label><figDesc>Figure 7: Evaluation prompt used with Llama 3 70B to annotate the predicted descriptions. The prompt structure is adapted towards description prediction from Alpaca-Eval. The positions are indicated by placeholders [. . .], where the ground truth and predictions to be annotated are inserted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We repurpose StoryBench, a video dataset with human-annotated descriptions, into StoryFrames, a corpus of 8,881 samples for facilitating research on generalpurpose paired sequential vision and text data.</figDesc><table /><note>• Context Length Ablations: We show that training across multiple context lengths consistently outperforms training on single length, suggesting that exposure to varied temporal spans enhances sequential reasoning.• StoryFrames:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Table 1 shows that our proposed IM-AGECHAIN model consistently achieves the highest SimRate across all evaluated context lengths, demonstrating the value of explicitly modeling a sequence of images for next-scene description task. Similarity Rate (SimRate, %) across varying evaluation context lengths for IMAGECHAIN and baseline models trained on C2-7. IMAGECHAIN consistently outperforms all baselines and achieves the highest SimRate across all evaluated contexts.</figDesc><table><row><cell>IMAGECHAIN significantly outperforms standard</cell></row><row><cell>fine-tuning (MLLM-FT), with a notable 15.39 per-</cell></row><row><cell>centage point improvement in SimRate on C6, the</cell></row><row><cell>longest evaluated context length. Additionally, IM-</cell></row><row><cell>AGECHAIN achieves 19.02% on C2-6, surpassing</cell></row><row><cell>VisualContext (13.01%) and FinalScene (9.71%),</cell></row><row><cell>underscoring the value of incorporating both im-</cell></row><row><cell>ages and textual description history. Even with-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Similarity rate (SimRate) on next-scene description when IMAGECHAIN is trained and evaluated on varying context lengths. ▲ refers to the gains from training on Cτ -7 compared to the corresponding Cτ trained model. IMAGECHAIN benefits from training on different context lengths (i.e., number of scenes).</figDesc><table><row><cell>quential reasoning. Additionally, when tested on</cell></row><row><cell>C3, IMAGECHAIN C2−7 demonstrates a substantial</cell></row><row><cell>improvement of 5.1 points (21.3% vs. 16.2%) over</cell></row><row><cell>the baseline IMAGECHAIN C3 model. Finally, on</cell></row><row><cell>the C4-6 test setting, IMAGECHAIN C2−7 , achieves</cell></row><row><cell>a 2.8 point increase in SimRate (16.8% vs. 14%).</cell></row><row><cell>This indicates that the sequential training strategy</cell></row><row><cell>across multiple context lengths enables stronger</cell></row><row><cell>reasoning over extended visual sequences.</cell></row><row><cell>out fine-tuning, IMAGECHAIN-NoFT outperforms</cell></row><row><cell>MLLM on C2-6 (7.11% vs. 3.70%), demonstrat-</cell></row><row><cell>ing the benefits of explicitly modeling image se-</cell></row><row><cell>quences. However, fine-tuning remains essential</cell></row><row><cell>for optimal performance, suggesting that MLLMs</cell></row><row><cell>lack strong inherent temporal reasoning skills with-</cell></row><row><cell>out additional training.</cell></row><row><cell>IMAGECHAIN Benefits from Training on Dif-</cell></row><row><cell>ferent Context Lengths. We train and evaluate</cell></row><row><cell>an IMAGECHAIN model for each context length,</cell></row><row><cell>with results shown in Table 2. We observe that IM-</cell></row><row><cell>AGECHAIN C2−7 model outperforms the baseline</cell></row><row><cell>IMAGECHAIN C2 model when tested on C2, achiev-</cell></row><row><cell>ing a 1.7 point improvement in SimRate (18.3% vs.</cell></row><row><cell>16.6%). This suggests that training on a broader</cell></row><row><cell>range of context lengths helps the model generalize</cell></row><row><cell>better, even when predicting in shorter contexts.</cell></row><row><cell>Similarly, we find that IMAGECHAIN C2−7 , which</cell></row><row><cell>is trained on the entire range of temporal spans,</cell></row><row><cell>consistently outperforms models trained on spe-</cell></row><row><cell>cific context lengths. This suggests that training</cell></row><row><cell>across multiple context lengths enables robust se-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Zero-shot out-of-domain performance comparison (F1 score) on Comics, Daily-Life (DL), and Robotics (Robo) datasets. IMAGECHAIN excels in structured, event-based reasoning tasks (comics and robotics), trailing Gemini only by 0.1 and 0.8 points, respectively. GPT-4V and Gemini results are taken from Wang et al. (2024c).</figDesc><table><row><cell>Model</cell><cell>Comics</cell><cell>DL</cell><cell>Robo</cell></row><row><cell>GPT-4V Gemini</cell><cell>18.1 16.3</cell><cell>33.6 21.6</cell><cell>34.0 39.4</cell></row><row><cell>MLLM MLLM-FT VisualContext FinalScene IMAGECHAIN</cell><cell>13.3 13.0 11.2 13.4 16.2</cell><cell>23.5 18.4 18.3 26.5 20.8</cell><cell>15.2 14.4 27.8 19.0 27.1</cell></row><row><cell cols="4">Comics. In the comics domain, IMAGECHAIN</cell></row><row><cell cols="4">trails Gemini by only 0.1 points (16.2 vs. 16.3) and</cell></row><row><cell cols="4">surpasses all 7B baselines, indicating that model-</cell></row><row><cell cols="4">ing sequential dependencies benefits structured vi-</cell></row><row><cell cols="4">sual narratives. Figure 6 shows how IMAGECHAIN</cell></row><row><cell cols="4">captures the evolution of events over time, unlike</cell></row><row><cell cols="4">MLLM-FT that focus on static observations.</cell></row><row><cell cols="4">Daily-Life. For daily life (DL) videos, IM-</cell></row><row><cell cols="4">AGECHAIN lags behind FinalScene (20.8 vs. 26.5</cell></row><row><cell cols="4">F1), suggesting that the task is closer to traditional</cell></row><row><cell cols="4">captioning since descriptions summarize videos</cell></row><row><cell cols="4">with fewer significant changes compared to comics,</cell></row><row><cell cols="4">where progression between frames is more explicit.</cell></row><row><cell cols="4">Robotics. In robotics (Robo), IMAGECHAIN</cell></row><row><cell cols="4">(27.1 F1) shows a substantial improvement over</cell></row><row><cell cols="4">MLLM (15.2 F1) and MLLM-FT (14.4 F1), high-</cell></row><row><cell cols="4">lighting that explicitly modeling image sequences</cell></row><row><cell cols="4">improves reasoning in multi-step environments,</cell></row><row><cell cols="4">even in zero-shot out-of-domain settings. Visual-</cell></row><row><cell cols="4">Context (27.8 F1) achieves a similar performance,</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code, dataset, and checkpoints are publicly available at https://github.com/danaesavi/ImageChain.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We follow Mementos evaluation setup<ref type="bibr" target="#b44">(Wang et al., 2024c)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research was supported by a research grant (VIL53122) from VILLUM FONDEN, and by the European Union's Horizon 2020 research and innovation program under grant agreement No. 101135671 (TrustLLM).</p><p>We acknowledge EuroHPC Joint Undertaking for awarding us access to Karolina, hosted by IT4Innovations, Czech Republic.</p><p>This work was supported by the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID:DD-24-66).</p><p>We thank Rita Ramos for interesting discussions on this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. 2023. GPT-4 Technical Report</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Storybench: A multifaceted benchmark for continuous story visualization</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hernan</forename><surname>Moraldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Taghi</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="78095" to="78125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation</title>
		<author>
			<persName><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Börschinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="291" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Event-content-oriented dialogue generation in short video</title>
		<author>
			<persName><forename type="first">Fenghua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.naacl-long.229</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<title level="m">Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* Chat-GPT Quality</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng ; Tianle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Banghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.04132</idno>
		<title level="m">Chatbot arena: An open platform for evaluating llms by human preference</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Anastasios Nikolas Angelopoulos</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The limits of n-gram translation evaluation metrics</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Culy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><forename type="middle">Z</forename><surname>Riehemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit IX: Papers</title>
				<meeting>Machine Translation Summit IX: Papers</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Tinystories: How small can language models be and still speak coherent english?</title>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.07759</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Oops! predicting unintentional action in video</title>
		<author>
			<persName><forename type="first">Dave</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="919" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">Gemini: a family of highly capable multimodal models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Diana</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Rico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gibran</forename><surname>Fuentes-Pineda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00738</idno>
		<title level="m">Contextualize, show and tell: A neural visual storyteller</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Long short-term memory</title>
		<author>
			<persName><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Neural Computation MIT-Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diverse and relevant visual storytelling with scene graph embeddings</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asad</forename><surname>Sayeed</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.conll-1.34</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Conference on Computational Natural Language Learning</title>
				<meeting>the 24th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="420" to="430" />
		</imprint>
	</monogr>
	<note>Khushboo Mehra, Vera Demberg, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Xiaodong He, Pushmeet Kohli</title>
		<author>
			<persName><forename type="first">Ting-Hao Kenneth</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick ; Dhruv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
				<meeting>the 2016 Conference of the North American Chapter<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1233" to="1239" />
		</imprint>
	</monogr>
	<note>Visual storytelling</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Fazli Imam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Fikri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aji</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2501.10674</idno>
		<title level="m">Can multimodal llms do visual temporal understanding and reasoning? the answer is no! Preprint</title>
				<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Limit: Less is more for instruction tuning across evaluation paradigms</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Havens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Dohmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Portes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Dongfu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaye</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.01483</idno>
		<title level="m">Mantis: Interleaved multi-image instruction tuning</title>
				<imprint>
			<date type="published" when="2024">2024a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Instruction-tuned language models are better knowledge learners</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.296</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5421" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Glac net: Glocal attention cascading networks for multiimage cued story generation</title>
		<author>
			<persName><forename type="first">Taehyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonil</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung-Wha</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10973</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What is more likely to happen next? videoand-language future event prediction</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.706</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8769" to="8784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">From generation to judgment: Opportunities and challenges of llm-as-a-judge</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alimohammad</forename><surname>Beigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengshuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.16594</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huixue</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.05579</idno>
		<title level="m">Llms-as-judges: A comprehensive survey on llm-based evaluation methods</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Alpacaeval: An automatic evaluator of instruction-following models</title>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/alpaca_eval" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving visual storytelling with multimodal large language models</title>
		<author>
			<persName><forename type="first">Xiaochuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.02586</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual storytelling with question-answer plans</title>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-emnlp.386</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
				<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="5800" to="5813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Llavanext: Improved reasoning, ocr, and world knowledge</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mm1: methods, analysis and insights from multimodal llm pre-training</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Mckinzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Fauconnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruti</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Futang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Belyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="304" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Validated image caption rating dataset</title>
		<author>
			<persName><forename type="first">Lothar</forename><forename type="middle">D</forename><surname>Narins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakash</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anagha</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mar</forename><surname>Castanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shasta</forename><surname>Ihorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue-Ting</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0</title>
		<author>
			<persName><forename type="first">Abby O'</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhiram</forename><surname>Maddukuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Padalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Acorn</forename><surname>Pooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajinkya</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6892" to="6903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022-Tenth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual question answering using deep learning: A survey and performance analysis</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnav</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Shiv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snehasis</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Processing: 5th International Conference</title>
				<meeting><address><addrLine>Prayagraj, India</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2021. December 4-6, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="75" to="86" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers, Part II 5</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">From show to tell: A survey on deep learning-based image captioning. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Cascianelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="539" to="559" />
		</imprint>
	</monogr>
	<note>Giuseppe Fiameni, and Rita Cucchiara</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mathscale: Scaling instruction tuning for mathematical reasoning</title>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Llamav-o1: Rethinking step-by-step visual reasoning in llms</title>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Thawakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinura</forename><surname>Dissanayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ketan</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritesh</forename><surname>Thawkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Heakl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noor</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zumri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.06186</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Instruct once, chat consistently in multiple rounds: An efficient tuning framework for dialogue</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chak</forename><forename type="middle">Tou</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongding</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.219</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3993" to="4010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Qwen2-vl: Enhancing vision-language model&apos;s perception of the world at any resolution</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12191</idno>
		<imprint>
			<date type="published" when="2024">2024b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Storytelling from an image stream using scene graphs</title>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9185" to="9192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unidentified video objects: A benchmark for dense, open-world segmentation</title>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
				<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10776" to="10785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences</title>
		<author>
			<persName><forename type="first">Xiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuancheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feihong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taixi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.25</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Papers; Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024c</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="416" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Modern question answering datasets and benchmarks: A survey</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.15030</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Next-qa: Next phase of questionanswering to explaining temporal actions</title>
		<author>
			<persName><forename type="first">Junbin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9777" to="9786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A survey on multi-turn interaction capabilities of large language models</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.09959</idno>
		<imprint>
			<date type="published" when="2025">2025a</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Duzhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yahan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.13601</idno>
		<title level="m">Mmllms: Recent advances in multimodal large language models</title>
				<imprint>
			<date type="published" when="2024">2024a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Let storytelling tell vivid stories: A multi-modalagent-based unified storytelling framework. Neurocomputing</title>
		<author>
			<persName><forename type="first">Rongsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingtao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025b</date>
			<biblScope unit="page">129316</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.10792</idno>
		<title level="m">struction tuning for large language models: A survey</title>
				<imprint>
			<date type="published" when="2024">2024b</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lima: Less is more for alignment</title>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2024">2024a</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Rethinking visual dependency in longcontext reasoning for large vision-language models</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.19732</idno>
		<imprint>
			<date type="published" when="2024-06">Jun Wan, and Jianbing Shen. 2024b</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
